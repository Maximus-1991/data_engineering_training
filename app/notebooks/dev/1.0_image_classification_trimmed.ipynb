{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on data engineering Vantage AI\n",
    "In this session we are going to train a convolutional neural network to classify images of the CIFAR-10 Dataset. \n",
    "\n",
    "## Assignment\n",
    "We would like this notebook defined as a clean package following the cookiecutter template. It should consist of a model training part, and a model scoring part (score on the test set).\n",
    "\n",
    "The training part stores a model and metadata on disk.  \n",
    "The scoring part uses the stored model to predict the testset and show the results.  \n",
    "Runnen with: `python train_model.py {path}` or `python score_model.py {path}`\n",
    "\n",
    "Use proper error handling, for example for making predictions without a model, or run with a faulty argument.\n",
    "\n",
    "\n",
    "## Dependency management\n",
    "This notebook expects you to have the following python dependencies installed:\n",
    "- Tensorflow (2.0)\n",
    "- Matplotlib\n",
    "- SKLearn\n",
    "\n",
    "Exercise: _Write a `requirements.txt` in which the dependencies of this notebook can be installed easily._\n",
    "\n",
    "## Load data\n",
    "\n",
    "The data consist of three parts: train, validation and test set.\n",
    "\n",
    "Exercise: _There is many repative code when loading data. Divide this in practical and readable code. Think about the engineering principles we discussed during the session_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # training set, batches 1-4\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), \"data\")):\n",
    "        os.makedirs(os.path.join(os.getcwd(), \"data\"))\n",
    "\n",
    "        \n",
    "    dataset_dir = os.path.join(os.getcwd(), \"data\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(dataset_dir, \"cifar-10-batches-py\")):\n",
    "        print(\"Downloading data...\")\n",
    "        urlretrieve(\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", os.path.join(dataset_dir, \"cifar-10-python.tar.gz\"))\n",
    "        tar = tarfile.open(os.path.join(dataset_dir, \"cifar-10-python.tar.gz\"))\n",
    "        tar.extractall(dataset_dir)\n",
    "        tar.close()\n",
    "        \n",
    "    X_train = np.zeros((40000, 32, 32, 3), dtype=\"float32\")\n",
    "    y_train = np.zeros((40000, 1), dtype=\"ubyte\").flatten()\n",
    "    n_samples = 10000  # aantal samples per batch\n",
    "    dataset_dir = os.path.join(dataset_dir,\"cifar-10-batches-py\")\n",
    "    for i in range(0,4):\n",
    "        f = open(os.path.join(dataset_dir, \"data_batch_\"+str(i+1)), \"rb\")\n",
    "        cifar_batch = pickle.load(f,encoding=\"latin1\")\n",
    "        f.close()\n",
    "        X_train[i*n_samples:(i+1)*n_samples] = (cifar_batch['data'].reshape(-1, 32, 32, 3) / 255.).astype(\"float32\")\n",
    "        y_train[i*n_samples:(i+1)*n_samples] = np.array(cifar_batch['labels'], dtype='ubyte')\n",
    "\n",
    "    # validation set, batch 5\n",
    "    f = open(os.path.join(dataset_dir, \"data_batch_5\"), \"rb\")\n",
    "    cifar_batch_5 = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    X_val = (cifar_batch_5['data'].reshape(-1, 32, 32, 3) / 255.).astype(\"float32\")\n",
    "    y_val = np.array(cifar_batch_5['labels'], dtype='ubyte')\n",
    "\n",
    "    # labels\n",
    "    f = open(os.path.join(dataset_dir, \"batches.meta\"), \"rb\")\n",
    "    cifar_dict = pickle.load(f,encoding=\"latin1\")\n",
    "    label_to_names = {k:v for k, v in zip(range(10), cifar_dict['label_names'])}\n",
    "    f.close()\n",
    "\n",
    "    # test set\n",
    "    f = open(os.path.join(dataset_dir, \"test_batch\"), \"rb\")\n",
    "    cifar_test = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    X_test = (cifar_test['data'].reshape(-1, 32, 32, 3) / 255.).astype(\"float32\")\n",
    "    y_test = np.array(cifar_test['labels'], dtype='ubyte')\n",
    "\n",
    "\n",
    "    print(\"training set size: data = {}, labels = {}\".format(X_train.shape, y_train.shape))\n",
    "    print(\"validation set size: data = {}, labels = {}\".format(X_val.shape, y_val.shape))\n",
    "    \n",
    "    print(\"Test set size: data = \"+str(X_test.shape)+\", labels = \"+str(y_test.shape))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, label_to_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocessing\n",
    "CIFAR-10 does not require much preprocessing. Normalizing data is often a good idea, we calculate the average pixel value in advance, we normalize the data based on that value. \n",
    "\n",
    "Exercise: _Its a good idea to store the mean and std in a pickle file, which can be used for inference so that we do not require to load the whole dataset._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "training set size: data = (40000, 32, 32, 3), labels = (40000,)\n",
      "validation set size: data = (10000, 32, 32, 3), labels = (10000,)\n",
      "Test set size: data = (10000, 32, 32, 3), labels = (10000,)\n"
     ]
    }
   ],
   "source": [
    "nr_channels = 3\n",
    "image_size = 32\n",
    "nr_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, label_to_names = load_data()\n",
    "\n",
    "# Conv nets trainen duurt erg lang op CPU, dus we gebruiken maar een klein deel\n",
    "# van de data nu, als er tijd over is kan je proberen je netwerk op de volledige set te runnen\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "def calc_mean_std(X):\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    return mean, std\n",
    "\n",
    "def normalize(data, mean, std):\n",
    "    return (data-mean)/std\n",
    "\n",
    "#De data van train_X is genoeg om de mean en std van de hele set nauwkeurig te benaderen\n",
    "mean,std = calc_mean_std(X_train)\n",
    "X_test = normalize(X_test,mean,std)\n",
    "X_val = normalize(X_val,mean,std)\n",
    "X_train = normalize(X_train ,mean,std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n",
    "We use a convolutional neural net as our model, which is very basic. Since the data science is not the focus of this course, we do not pay much attention to this. It is good to know what is going on though, so don't hesistate to ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "10000/10000 - 2s - loss: 1.7349 - accuracy: 0.3864 - val_loss: 1.5719 - val_accuracy: 0.4505\n",
      "Accuracy = 0.4529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.41      0.73      0.52      1000\n",
      "  automobile       0.57      0.46      0.51      1000\n",
      "        bird       0.41      0.32      0.36      1000\n",
      "         cat       0.37      0.20      0.26      1000\n",
      "        deer       0.40      0.41      0.40      1000\n",
      "         dog       0.35      0.50      0.41      1000\n",
      "        frog       0.49      0.53      0.51      1000\n",
      "       horse       0.46      0.50      0.48      1000\n",
      "        ship       0.72      0.38      0.50      1000\n",
      "       truck       0.53      0.50      0.52      1000\n",
      "\n",
      "    accuracy                           0.45     10000\n",
      "   macro avg       0.47      0.45      0.45     10000\n",
      "weighted avg       0.47      0.45      0.45     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, Input\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def conv_net():\n",
    "    input_layer = Input(shape=X_train.shape[1:])\n",
    "    conv = Conv2D(filters=16, kernel_size=(3,3), padding='valid',\n",
    "                  data_format='channels_last', activation='relu')(input_layer)\n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='valid',\n",
    "                  data_format='channels_last', activation='relu', strides=(2, 2))(conv)\n",
    "\n",
    "    flatten = Flatten()(conv)\n",
    "    output_layer = Dense(units=nr_classes, activation='softmax')(flatten)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])   \n",
    "    return model\n",
    "\n",
    "\n",
    "model = conv_net()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=50, epochs=1, validation_data=(X_val, y_val), verbose=2)\n",
    "predictions = np.array(model.predict(X_test, batch_size=100))\n",
    "test_y = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "#Take the highest prediction\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Store model on disk\n",
    "model.save('model.h5')\n",
    "\n",
    "#Print results\n",
    "print(\"Accuracy = {}\".format(np.sum(predictions == y_test) / float(len(predictions))))\n",
    "print(classification_report(y_test, predictions, target_names=list(label_to_names.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 67,818\n",
      "Trainable params: 67,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
